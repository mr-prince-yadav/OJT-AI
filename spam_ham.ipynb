{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install wordcloud\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv(\"/content/spam.csv\",encoding = 'latin-1')\n",
    "print(data.head())\n",
    "data.shape\n",
    "data=data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "print(data)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "cols= [\"#98568d\", \"#53fca1\"]\n",
    "plt.figure(figsize=(10,5))\n",
    "fg=sns.countplot(x=data['v1'],legend = False,palette=cols)\n",
    "fg.set_title(\"Count plot of classes\")\n",
    "fg.set_xlabel(\"Classes\")\n",
    "fg.set_ylabel(\"Number of data points\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "data[\"No_of_Characters\"] = data['v2'].apply(len)\n",
    "data[\"No_of_Words\"]=data.apply(lambda row: nltk.word_tokenize(row['v2']), axis=1).apply(len)\n",
    "data[\"No_of_sentence\"]=data.apply(lambda row: nltk.sent_tokenize(row['v2']), axis=1).apply(len)\n",
    "data.describe().T\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "fg= sns.pairplot(data=data,hue=\"v1\",palette=cols)\n",
    "plt.show(fg)\n",
    "\n",
    "#dropping the outliers\n",
    "data=data[(data['No_of_Characters']<350)]\n",
    "data.shape\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "fg=sns.pairplot(data=data,hue='v1',palette=cols)\n",
    "plt.show(fg)\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "word_cloud=WordCloud(width =800,height=800,max_words=200,stopwords=stop_word,background_color='black',max_font_size=200)\n",
    "spam = data.query(\"v1=='spam'\").v2.str.cat(sep=' ')\n",
    "ham = data.query(\"v1=='ham'\").v2.str.cat(sep=' ')\n",
    "\n",
    "print(\"Spam\")\n",
    "word_cloud.generate(spam)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(word_cloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ham\")\n",
    "word_cloud.generate(ham)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(word_cloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The first 5 texts:\",*data['v2'][:5],sep=\"\\n\")\n",
    "\n",
    "#defining a function to clean the text\n",
    "import re\n",
    "def Cleaning(v2):\n",
    "  sms=re.sub('[^a-zA-Z]',' ',v2) #replacing all non-alphabetic character with space\n",
    "  sms=sms.lower()\n",
    "  sms=sms.split()\n",
    "  sms=' '.join(sms)\n",
    "  return sms\n",
    "data[\"clean_text\"]=data[\"v2\"].apply(Cleaning)\n",
    "print(\"The first 5 text after cleaning \",*data['clean_text'][:5],sep='\\n')\n",
    "\n",
    "#tokenization\n",
    "data[\"Tokenize_text\"]=data.apply(lambda row: nltk.word_tokenize(row['clean_text']),axis=1)\n",
    "print(data[\"Tokenize_text\"])\n",
    "print(\"The first 5 text after tokenizing: \",*data['Tokenize_text'][:5],sep='\\n')\n",
    "\n",
    "#Removing stopwords function\n",
    "def remove_stopword(v2):\n",
    "  stop_word=set(stopwords.words('english'))\n",
    "  filtered_text=[]\n",
    "  for word in v2:\n",
    "    if word not in stop_word:\n",
    "      filtered_text.append(word)\n",
    "  return filtered_text\n",
    "data[\"Nostopwords\"]=data[\"Tokenize_text\"].apply(remove_stopword)\n",
    "print(\"First 5 text after removing stopword: \",*data['Nostopwords'][:5],sep='\\n')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_word(v2):\n",
    "    lemmas = []\n",
    "    for word in v2:\n",
    "      lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "      lemmas.append(lemma)\n",
    "    #lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in v2]\n",
    "    return lemmas\n",
    "\n",
    "data[\"Lemmatized_text\"] = data[\"Nostopwords\"].apply(lemmatize_word)\n",
    "print(\"The First 5 Texts after lemitization: \",*data[\"Lemmatized_text\"][:5], sep = \"\\n\")\n",
    "\n",
    "#Creating a corpus of text feature to encode further into vectorized form\n",
    "corpus=[]\n",
    "for i in data['Lemmatized_text']:\n",
    "  msg = ' '.join([row for row in i])\n",
    "  corpus.append(msg)\n",
    "corpus[:5]\n",
    "print(\"The first 5 lines in corpus : \",*corpus[:5],sep='\\n')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x=tfidf.fit_transform(corpus).toarray()\n",
    "x.dtype\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "data['v1']=label_encoder.fit_transform(data['v1'])\n",
    "#print(data.head)\n",
    "\n",
    "y=data['v1']\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "print(\"Traning set shape \",x_train.shape)\n",
    "print(\"testing set shape \",x_test.shape)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "svm_model=SVC(kernel='linear',random_state=42)\n",
    "svm_model.fit(x_train,y_train)\n",
    "y_pred=svm_model.predict(x_test)\n",
    "accuracy_svm=accuracy_score(y_test,y_pred)\n",
    "classification=classification_report(y_test,y_pred,target_names=['Ham','Spam'])\n",
    "print(\"Accuracy\",accuracy_svm)\n",
    "print(\"Classification Report :\\n \",classification)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC:\", roc_auc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
